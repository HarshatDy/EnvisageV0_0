# Envisage Project: Deep Technical Description

## Overview
The Envisage project is a modular, automated news aggregation, summarization, and image enrichment pipeline. It orchestrates multiple scripts to fetch, process, summarize, and visually enhance news data, storing results in both MongoDB and Google Cloud Storage. The system is designed for scheduled, robust, and scalable operation, leveraging multi-threading, cloud APIs, and advanced scraping techniques.

## Main Components and Workflow

### 1. **Scheduler (Envisage_Schedule.py)**
- **Role:** Orchestrates the entire pipeline, running a sequence of scripts at scheduled times (3:00 AM and 2:45 PM daily) or on demand.
- **Mechanism:** Uses the `schedule` library to trigger the following scripts in strict order, ensuring each step succeeds before proceeding.
- **Logging:** Centralized logging to both file and stdout for traceability.

### 2. **Worker Thread (worker_thread.py)**
- **Role:** Initiates the news retrieval and summarization process.
- **Mechanism:**
  - Uses multi-threading (`threading.Thread`) to parallelize checks and summarization tasks.
  - Interfaces with the `GeminiAPI` (from `gemini_api_test_time_based_scrapper_gemini.py`) to fetch news, check for updates, and run summarization.
  - Logs progress and results.
- **DSA/Algorithms:**
  - Thread management for concurrent execution.
  - Conditional logic for task orchestration.

### 3. **Web Content Controller (envisage_web_ct_ctr.py)**
- **Role:** Transforms summarized news data for web presentation.
- **Mechanism:**
  - Fetches summaries from MongoDB, cleans and structures them for web display.
  - Handles date-based constraints for news cycles (morning/evening logic).
  - Cleans and normalizes text data.
- **DSA/Algorithms:**
  - Dictionary and list manipulation for data transformation.
  - Regular expressions for text cleaning.

### 4. **Thumbnail Scraper (web_scrapper_thumbnail.py)**
- **Role:** Enriches news categories with relevant images.
- **Mechanism:**
  - Uses multi-threading and thread pools (`ThreadPoolExecutor`) for concurrent image search and download.
  - Employs multiple scraping approaches (with retry logic, proxy rotation, and user-agent spoofing) to maximize image retrieval success.
  - Integrates with external APIs (e.g., Google Gemini, Unsplash, Pexels) for image search.
  - Stores images in a structured local directory by date and category.
- **DSA/Algorithms:**
  - Thread-safe counters and locks for concurrency control.
  - Queue and thread pool for parallel task execution.
  - Retry and fallback strategies for robust scraping.
  - Randomization for user-agent and proxy selection.

### 5. **Thumbnail Push & DB Update (web_scrapper_thumbnail_push.py)**
- **Role:** Uploads images to Google Cloud Storage and updates MongoDB with image URLs.
- **Mechanism:**
  - Scans local image directories, uploads to GCS using the `google.cloud.storage` API.
  - Retrieves public URLs and updates corresponding MongoDB documents, matching by article/category.
  - Supports command-line arguments for different modes (push, update-db).
- **DSA/Algorithms:**
  - Directory traversal and file I/O.
  - String parsing and regular expressions for mapping images to articles.
  - Dictionary mapping for associating images with news items.

### 6. **Gemini API Integration (gemini_api_test_time_based_scrapper_gemini.py)**
- **Role:** Central to news retrieval, categorization, and summarization.
- **Mechanism:**
  - Uses Google Gemini and OpenAI APIs for content extraction, categorization, and summarization.
  - Multi-threaded processing for fetching and summarizing articles.
  - Stores results in MongoDB with date-based keys.
- **DSA/Algorithms:**
  - Threading and locks for safe concurrent operations.
  - Batch processing and retry logic for robust data fetching.
  - Dictionary-based data modeling for news, categories, and summaries.

### 7. **Database Access (mongo.py, sql_connection.py)**
- **MongoDB:** Used for storing all news, summaries, and image metadata. Connection managed via `pymongo`.
- **MySQL:** (Optional) Used for category metadata, accessed via `mysql.connector`.
- **DSA/Algorithms:**
  - Standard CRUD operations.
  - Dictionary/list structures for data interchange.

## Data Structures & Algorithms (DSA)
- **Threading:** Used extensively for parallelizing I/O-bound tasks (news fetching, summarization, image scraping).
- **Queue/ThreadPool:** For managing concurrent scraping and downloading tasks.
- **Dictionaries/Lists:** Core data modeling for news items, categories, summaries, and image metadata.
- **Regular Expressions:** For text cleaning, parsing, and mapping.
- **Retry/Resilience Patterns:** For robust scraping and API interaction (exponential backoff, fallback strategies).
- **Randomization:** For user-agent/proxy rotation to avoid scraping bans.

## Pipeline Flow
1. **Scheduled Trigger:** `Envisage_Schedule.py` starts the pipeline.
2. **News Fetch & Summarization:** `worker_thread.py` (via `GeminiAPI`) fetches, categorizes, and summarizes news.
3. **Web Data Structuring:** `envisage_web_ct_ctr.py` transforms summaries for web use.
4. **Image Enrichment:** `web_scrapper_thumbnail.py` scrapes and downloads relevant images.
5. **Image Upload & DB Update:** `web_scrapper_thumbnail_push.py` uploads images to GCS and updates MongoDB with image URLs.
6. **All steps are logged and can be debugged via generated log files.**

## Extensibility & Robustness
- **Modular Design:** Each script is independently runnable and testable.
- **Environment Variables:** Used for API keys, DB credentials, and cloud config for security and flexibility.
- **Error Handling:** Extensive try/except blocks, logging, and fallback logic for resilience.
- **Cloud Integration:** Google Cloud Storage for scalable image hosting.

## Conclusion
The Envisage project is a robust, scalable, and extensible news aggregation and enrichment pipeline, leveraging advanced multi-threading, cloud APIs, and resilient scraping techniques. Its modular architecture allows for easy extension and maintenance, while its use of modern DSA and algorithms ensures efficiency and reliability at scale. 